{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be0d6526-6fb7-4cb6-b4a9-e44f07b33d10",
   "metadata": {},
   "source": [
    "# NAFNet GAN to denoise images\n",
    "\n",
    "This is my implementation trained on an AMD Ryzen 7 5800X / 32GB RAM / RTX 5060 Ti 16GB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2e64ad-b578-46d0-a823-a04af2eeb409",
   "metadata": {},
   "source": [
    "### Step 0: Import all the libraries and codes necessary to execute the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14a8c05-61ba-4f04-85e8-761712c95fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6c5e1e-e99e-4801-9d37-10bbe657b76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.amp import GradScaler, autocast\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Code imports\n",
    "from dataloader import DenoisingDataset2D\n",
    "from models import NAFNet\n",
    "from models import Deep_Discriminator as Discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf13b91-f878-4d33-8414-3d1253918542",
   "metadata": {},
   "source": [
    "### Step 1: Define paths to load the images.\n",
    "\n",
    "The dataset has images that are between 64x64 and 1024x1024. By default, the cropping was set to 64. If you want to tweak the cropping, just change the parameter 'crop_size'. Note that this will only be used on train and not on validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ba3668-e7dd-442d-ae01-1c9c54de6577",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(noisy, pred, target, epoch, save_dir=\"training_visuals\"):\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(12, 5))\n",
    "    titles = [\"Noisy Input\", \"Denoised Output\", \"Ground Truth\"]\n",
    "    for i, img in enumerate([noisy, pred, target]):\n",
    "        img = img.squeeze().cpu().numpy()  # Shape: (H, W)\n",
    "        img = np.clip(img, 0, 1)\n",
    "        axs[i].imshow(img, cmap='gray')\n",
    "        axs[i].set_title(titles[i])\n",
    "        axs[i].axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    plt.savefig(os.path.join(save_dir, f\"epoch_{epoch}_viz.png\"))\n",
    "    plt.close()  # Close the plot to free memory\n",
    "\n",
    "def load_data(train_dir, val_dir, test_size, batch_size, num_workers_tr, num_workers_val, crop_size = None):\n",
    "    train_noisy = sorted([os.path.join(train_dir, \"RAW\", f) for f in os.listdir(os.path.join(train_dir, \"RAW\")) if f.endswith('.tif')])\n",
    "    train_gt = sorted([os.path.join(train_dir, \"GT\", f) for f in os.listdir(os.path.join(train_dir, \"GT\")) if f.endswith('.tif')])\n",
    "    val_noisy = sorted([os.path.join(val_dir, \"RAW\", f) for f in os.listdir(os.path.join(val_dir, \"RAW\")) if f.endswith('.tif')])\n",
    "    val_gt = sorted([os.path.join(val_dir, \"GT\", f) for f in os.listdir(os.path.join(val_dir, \"GT\")) if f.endswith('.tif')])\n",
    "\n",
    "    print(f\"{len(train_noisy)} training images and {len(val_noisy)} validation images after split.\")\n",
    "\n",
    "    train_ds = DenoisingDataset2D(train_noisy, train_gt, crop_size=crop_size, augment=True)\n",
    "    val_ds = DenoisingDataset2D(val_noisy, val_gt, crop_size=crop_size, augment=False)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers_tr, pin_memory=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=32, shuffle=False, num_workers=num_workers_val, pin_memory=True) # batch_size = 1\n",
    "\n",
    "    print(f\"\\nTraining DataLoader created with {len(train_loader)} batches.\")\n",
    "    print(f\"Validation DataLoader created with {len(val_loader)} batches.\")\n",
    "\n",
    "    return train_loader, val_loader\n",
    "\n",
    "# Define data paths (update these to your actual paths)\n",
    "train_dir = r\"D:\\Manuscipts_Coding\\Denoising_paper\\IgG-1D\\Exported_Data_TIFF\\train\"\n",
    "val_dir = r\"D:\\Manuscipts_Coding\\Denoising_paper\\IgG-1D\\Exported_Data_TIFF\\val\"\n",
    "pretrained_path = \"DATASET_7_NAFNet_GAN_best_model_LOSS_2_V2.pth\"  # If it does not exist, then it will not load the weights on train.\n",
    "checkpoint_path = \"DATASET_7_NAFNet_GAN_best_model_LOSS_2_V2.pth\"\n",
    "crop_size = 128\n",
    "num_workers_tr = 0 # Number of workers for training \n",
    "num_workers_val = 0 # Number of workers for validation \n",
    "\n",
    "# Step 1: Load data\n",
    "if os.path.exists(train_dir) == True and os.path.exists(val_dir) == True:\n",
    "    train_loader, val_loader = load_data(\n",
    "        train_dir=train_dir,\n",
    "        val_dir=val_dir,\n",
    "        test_size=0.04,\n",
    "        batch_size=32,\n",
    "        num_workers_tr=num_workers_tr,  \n",
    "        num_workers_val=num_workers_val,  \n",
    "        crop_size=crop_size\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03882ee-680c-465d-96af-c82c25ae2614",
   "metadata": {},
   "source": [
    "### Step 2: Visualize the dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca94cf1e-476a-4593-a278-08b8e03e02f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Visualize a sample from training and validation data\n",
    "def visualize_dataloader(loader, title=\"Sample\"):\n",
    "    start_time = time.perf_counter()\n",
    "    noisy_img_batch, gt_img_batch = next(iter(loader))\n",
    "    end_time = time.perf_counter()\n",
    "    print(f\"Time to load first batch: {end_time - start_time:.4f} seconds\")\n",
    "    print(f\"Noisy batch shape: {noisy_img_batch.shape}, GT batch shape: {gt_img_batch.shape}\")\n",
    "\n",
    "    noisy_sample = noisy_img_batch[0].squeeze().cpu().numpy()\n",
    "    gt_sample = gt_img_batch[0].squeeze().cpu().numpy()\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    im0 = axs[0].imshow(noisy_sample, cmap='gray')\n",
    "    axs[0].set_title(f\"Noisy Input {title}\")\n",
    "    axs[0].axis(\"off\")\n",
    "    fig.colorbar(im0, ax=axs[0], fraction=0.046, pad=0.04)\n",
    "    im1 = axs[1].imshow(gt_sample, cmap='gray')\n",
    "    axs[1].set_title(f\"Ground Truth {title}\")\n",
    "    axs[1].axis(\"off\")\n",
    "    fig.colorbar(im1, ax=axs[1], fraction=0.046, pad=0.04)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"Visualizing training data sample:\")\n",
    "visualize_dataloader(train_loader, title=\"Training Sample (Cropped)\")\n",
    "\n",
    "print(\"Visualizing validation data sample:\")\n",
    "visualize_dataloader(val_loader, title=\"Validation Sample (Full)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fa12f9-bb02-46b4-bec2-4a86fcb2fdeb",
   "metadata": {},
   "source": [
    "### Step 3: Train the model.\n",
    "\n",
    "Note that we have only trained it for 1 epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3273a64-e892-402a-bd61-36f15d0fab0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from losses import MasterLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f873946b-189e-48e4-a40c-7a9f21d94fb2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TORCH_COMPILE_DISABLE'] = '1'\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.cuda.amp import GradScaler \n",
    "from torch.amp import autocast \n",
    "import warnings\n",
    "import time\n",
    "\n",
    "# --- Setup & Helper Functions ---\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def plot_images(noisy, pred, target, epoch, save_dir=\"training_visuals\"):\n",
    "    \"\"\"Saves and displays the input, predicted, and ground truth images with colorbars.\"\"\"\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    titles = [\"Noisy Input\", \"Denoised Output\", \"Ground Truth\"]\n",
    "    \n",
    "    # Move to CPU and detach for plotting\n",
    "    images = [\n",
    "        noisy.squeeze().cpu().detach().numpy(),\n",
    "        pred.squeeze().cpu().detach().numpy(),\n",
    "        target.squeeze().cpu().detach().numpy()\n",
    "    ]\n",
    "    \n",
    "    for i, img in enumerate(images):\n",
    "        im = axs[i].imshow(img, cmap='gray', vmin=0, vmax=1)\n",
    "        axs[i].set_title(f\"{titles[i]} (Epoch {epoch})\")\n",
    "        axs[i].axis(\"off\")\n",
    "        fig.colorbar(im, ax=axs[i], fraction=0.046, pad=0.04)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Assuming NAFNet, MasterLoss, LossWeights, train_loader, val_loader are defined/imported\n",
    "# If running as standalone script, ensure imports for these are present.\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- Model Setup ---\n",
    "img_channel = 1  # Grayscale\n",
    "width = 16\n",
    "enc_blks = [2, 2, 4, 8]\n",
    "middle_blk_num = 12\n",
    "dec_blks = [2, 2, 2, 2]\n",
    "\n",
    "# Initialize Generator (NAFNet)\n",
    "generator = NAFNet(\n",
    "    img_channel=img_channel,\n",
    "    width=width,\n",
    "    middle_blk_num=middle_blk_num,\n",
    "    enc_blk_nums=enc_blks,\n",
    "    dec_blk_nums=dec_blks\n",
    ").to(device)\n",
    "\n",
    "# Initialize Discriminator\n",
    "discriminator = Discriminator(in_channels=1).to(device)\n",
    "\n",
    "# Paths\n",
    "checkpoint_path = 'NAFNet_GAN_LVUP_Dataset_7_Conf-het_Best_Loss_3.pth'\n",
    "os.makedirs(\"training_visuals\", exist_ok=True)\n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs = 30\n",
    "T_max = max(1, int(num_epochs / 5)) \n",
    "\n",
    "# Optimizers & Schedulers\n",
    "g_optimizer = optim.Adam(generator.parameters(), lr=1e-4, betas=(0.5, 0.999))\n",
    "d_optimizer = optim.Adam(discriminator.parameters(), lr=1e-4, betas=(0.5, 0.999))\n",
    "\n",
    "g_scheduler = CosineAnnealingLR(g_optimizer, T_max=T_max)\n",
    "d_scheduler = CosineAnnealingLR(d_optimizer, T_max=T_max)\n",
    "\n",
    "# --- Loss Configuration ---\n",
    "# IMPORTANT: Using 'enhanced_deblur' for GAN training\n",
    "LOSS_TYPE = 'enhanced_deblur' \n",
    "\n",
    "class LossWeights:\n",
    "    lambda_lpips = 3.00    \n",
    "    lambda_vgg = 0.00     \n",
    "    lambda_charb = 0.5    \n",
    "    lambda_ssim = 0.0     \n",
    "    lambda_lap = 2.00     \n",
    "    lambda_edge = 0.0     \n",
    "    lambda_fft_cc = 0.5 \n",
    "    lambda_fft = 0.0     \n",
    "    lambda_gan = 0.5     \n",
    "    r1_gamma = 0.0       \n",
    "\n",
    "criterion = MasterLoss(loss_type=LOSS_TYPE, weights=LossWeights(), device=device)\n",
    "\n",
    "# Scalers for Mixed Precision\n",
    "scaler_g = GradScaler()\n",
    "scaler_d = GradScaler()\n",
    "\n",
    "# Tracking\n",
    "best_val_loss = float('inf')\n",
    "train_g_losses = []\n",
    "train_d_losses = []\n",
    "val_g_losses = []\n",
    "\n",
    "# --- PRE-TRAINING SETUP: Fixed Validation Batch ---\n",
    "# Grab one batch to use for consistent visualization throughout training\n",
    "print(\"Creating fixed validation batch for consistency...\")\n",
    "try:\n",
    "    fixed_val_input, fixed_val_target = next(iter(val_loader))\n",
    "    fixed_val_input = fixed_val_input.to(device)\n",
    "    fixed_val_target = fixed_val_target.to(device)\n",
    "    print(\"Fixed batch created successfully.\")\n",
    "except StopIteration:\n",
    "    print(\"Error: Validation loader is empty!\")\n",
    "    exit()\n",
    "\n",
    "# --- TRAINING LOOP ---\n",
    "for epoch in range(num_epochs):\n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "    \n",
    "    current_g_loss = 0.0\n",
    "    current_d_loss = 0.0\n",
    "    valid_batches = 0\n",
    "\n",
    "    # Train Step\n",
    "    pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Train]')\n",
    "    for noisy_img, clean_img in pbar:\n",
    "        noisy_img, clean_img = noisy_img.to(device), clean_img.to(device)\n",
    "\n",
    "        # NaN Check\n",
    "        if torch.isnan(noisy_img).any() or torch.isinf(noisy_img).any():\n",
    "            continue\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "        d_optimizer.zero_grad()\n",
    "        with autocast(device_type=device.type):\n",
    "            fake_img = generator(noisy_img)\n",
    "            fake_img = torch.clamp(fake_img, 0, 1) # Enforce [0,1] range\n",
    "            \n",
    "            # Real vs Fake inputs\n",
    "            d_real = discriminator(clean_img)\n",
    "            d_fake = discriminator(fake_img.detach()) # Detach to stop gradient to Generator\n",
    "            \n",
    "            d_loss = criterion.forward_discriminator(d_real, d_fake)\n",
    "\n",
    "        if torch.isnan(d_loss) or torch.isinf(d_loss):\n",
    "            continue\n",
    "\n",
    "        scaler_d.scale(d_loss).backward()\n",
    "        torch.nn.utils.clip_grad_norm_(discriminator.parameters(), max_norm=1.0)\n",
    "        scaler_d.step(d_optimizer)\n",
    "        scaler_d.update()\n",
    "\n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "        g_optimizer.zero_grad()\n",
    "        with autocast(device_type=device.type):\n",
    "            # Re-compute D output for Generator update (gradients flow this time)\n",
    "            d_fake_for_g = discriminator(fake_img)\n",
    "            \n",
    "            # Dictionary inputs for MasterLoss\n",
    "            g_loss_inputs = {\n",
    "                'pred_img': fake_img,\n",
    "                'target_img': clean_img,\n",
    "                'd_fake_logits': d_fake_for_g\n",
    "            }\n",
    "            g_loss = criterion.forward_generator(g_loss_inputs)\n",
    "\n",
    "        if torch.isnan(g_loss) or torch.isinf(g_loss):\n",
    "            continue\n",
    "\n",
    "        scaler_g.scale(g_loss).backward()\n",
    "        torch.nn.utils.clip_grad_norm_(generator.parameters(), max_norm=1.0)\n",
    "        scaler_g.step(g_optimizer)\n",
    "        scaler_g.update()\n",
    "\n",
    "        current_g_loss += g_loss.item()\n",
    "        current_d_loss += d_loss.item()\n",
    "        valid_batches += 1\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({'G_Loss': g_loss.item(), 'D_Loss': d_loss.item()})\n",
    "\n",
    "    # Epoch Averages\n",
    "    if valid_batches > 0:\n",
    "        current_g_loss /= valid_batches\n",
    "        current_d_loss /= valid_batches\n",
    "    else:\n",
    "        current_g_loss = float('inf')\n",
    "        current_d_loss = float('inf')\n",
    "\n",
    "    # --- VALIDATION LOOP ---\n",
    "    generator.eval()\n",
    "    discriminator.eval()\n",
    "    current_val_g_loss = 0.0\n",
    "    valid_val_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        with autocast(device_type=device.type):\n",
    "            for val_input, val_target in tqdm(val_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Valid]'):\n",
    "                val_input = val_input.to(device)\n",
    "                val_target = val_target.to(device)\n",
    "                \n",
    "                val_output = generator(val_input)\n",
    "                val_output = torch.clamp(val_output, 0, 1)\n",
    "                \n",
    "                # We only care about G loss for validation metric\n",
    "                d_fake_val = discriminator(val_output)\n",
    "                \n",
    "                val_inputs = {\n",
    "                    'pred_img': val_output,\n",
    "                    'target_img': val_target,\n",
    "                    'd_fake_logits': d_fake_val\n",
    "                }\n",
    "                \n",
    "                val_loss = criterion.forward_generator(val_inputs)\n",
    "                \n",
    "                if not (torch.isnan(val_loss) or torch.isinf(val_loss)):\n",
    "                    current_val_g_loss += val_loss.item() * val_input.size(0)\n",
    "                    valid_val_batches += val_input.size(0)\n",
    "\n",
    "    if valid_val_batches > 0:\n",
    "        current_val_g_loss /= valid_val_batches\n",
    "    else:\n",
    "        current_val_g_loss = float('inf')\n",
    "\n",
    "    # --- VISUALIZATION (Using Fixed Batch) ---\n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        generator.eval()\n",
    "        with torch.no_grad():\n",
    "            # Use the SAME fixed batch we grabbed at start\n",
    "            fixed_pred = generator(fixed_val_input)\n",
    "            fixed_pred = torch.clamp(fixed_pred, 0, 1)\n",
    "            \n",
    "            # Plot the first image from the fixed batch\n",
    "            plot_images(fixed_val_input[0], fixed_pred[0], fixed_val_target[0], epoch+1)\n",
    "\n",
    "    # --- LOGGING & SAVING ---\n",
    "    train_g_losses.append(current_g_loss)\n",
    "    train_d_losses.append(current_d_loss)\n",
    "    val_g_losses.append(current_val_g_loss)\n",
    "\n",
    "    print(f'Epoch {epoch + 1:02d} | Train G: {current_g_loss:.4f} | Train D: {current_d_loss:.4f} | Val G: {current_val_g_loss:.4f}')\n",
    "\n",
    "    # Save Best Model\n",
    "    if current_val_g_loss < best_val_loss and valid_val_batches > 0:\n",
    "        best_val_loss = current_val_g_loss\n",
    "        print(f\"--> New Best Model! Loss: {best_val_loss:.4f}\")\n",
    "        torch.save({\n",
    "            'generator_state_dict': generator.state_dict(),\n",
    "            'discriminator_state_dict': discriminator.state_dict(),\n",
    "            'g_optimizer_state_dict': g_optimizer.state_dict(),\n",
    "            'd_optimizer_state_dict': d_optimizer.state_dict(),\n",
    "            'val_loss': best_val_loss,\n",
    "        }, checkpoint_path)\n",
    "\n",
    "    # Save Latest Checkpoint\n",
    "    torch.save({\n",
    "        'generator_state_dict': generator.state_dict(),\n",
    "        'discriminator_state_dict': discriminator.state_dict(),\n",
    "        'g_optimizer_state_dict': g_optimizer.state_dict(),\n",
    "        'd_optimizer_state_dict': d_optimizer.state_dict(),\n",
    "        'val_loss': current_val_g_loss,\n",
    "    }, 'NAFNet_GAN_LVUP_Dataset_7_Conf-het_Latest_Loss_3.pth')\n",
    "    \n",
    "    g_scheduler.step()\n",
    "    d_scheduler.step()\n",
    "\n",
    "# --- FINISH & SAVE CSV ---\n",
    "loss_df = pd.DataFrame({\n",
    "    'Epoch': range(1, num_epochs + 1),\n",
    "    'Train Generator Loss': train_g_losses,\n",
    "    'Train Discriminator Loss': train_d_losses,\n",
    "    'Val Generator Loss': val_g_losses\n",
    "})\n",
    "loss_df.to_csv('NAFNet_GAN_LVUP_Dataset_7_Conf-het_Loss_3.csv', index=False)\n",
    "print(\"Training Complete.\")\n",
    "# Final visualization\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "generator.load_state_dict(checkpoint['generator_state_dict'])\n",
    "generator.eval()\n",
    "\n",
    "val_input, val_target = next(iter(val_loader))\n",
    "val_input = val_input.to(device)\n",
    "val_target = val_target.to(device)\n",
    "with torch.no_grad():\n",
    "    val_output = generator(val_input)\n",
    "\n",
    "input_img = val_input[0].cpu()\n",
    "pred_img = val_output[0].cpu()\n",
    "target_img = val_target[0].cpu()\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title(\"Input Image (Noisy)\")\n",
    "plt.imshow(input_img.squeeze(), cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title(\"Predicted Image (Denoised)\")\n",
    "plt.imshow(pred_img.squeeze(), cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title(\"Ground Truth (Clean)\")\n",
    "plt.imshow(target_img.squeeze(), cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.savefig(os.path.join(\"training_visuals\", \"final_result.png\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dbb84a-b6de-4c1d-aff3-5fae79b8d32a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad06fd80-e9ba-4c9b-b98b-434e4d871146",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f591961-a933-48f1-b421-7d5a474c3236",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e8e10a-1ad0-4185-940c-d6fb91aaf4ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

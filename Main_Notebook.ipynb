{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be0d6526-6fb7-4cb6-b4a9-e44f07b33d10",
   "metadata": {},
   "source": [
    "# NAFNet GAN to denoise images\n",
    "\n",
    "This is my implementation trained on an AMD Ryzen 7 5800X / 32GB RAM / RTX 5060 Ti 16GB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2e64ad-b578-46d0-a823-a04af2eeb409",
   "metadata": {},
   "source": [
    "### Step 0: Import all the libraries and codes necessary to execute the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14a8c05-61ba-4f04-85e8-761712c95fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6c5e1e-e99e-4801-9d37-10bbe657b76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.amp import GradScaler, autocast\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Code imports\n",
    "from dataloader import DenoisingDataset2D\n",
    "from models import NAFNet\n",
    "from models import Deep_Discriminator as Discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf13b91-f878-4d33-8414-3d1253918542",
   "metadata": {},
   "source": [
    "### Step 1: Define paths to load the images.\n",
    "\n",
    "The dataset has images that are between 64x64 and 1024x1024. By default, the cropping was set to 64. If you want to tweak the cropping, just change the parameter 'crop_size'. Note that this will only be used on train and not on validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ba3668-e7dd-442d-ae01-1c9c54de6577",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(noisy, pred, target, epoch, save_dir=\"training_visuals\"):\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(12, 5))\n",
    "    titles = [\"Noisy Input\", \"Denoised Output\", \"Ground Truth\"]\n",
    "    for i, img in enumerate([noisy, pred, target]):\n",
    "        img = img.squeeze().cpu().numpy()  # Shape: (H, W)\n",
    "        img = np.clip(img, 0, 1)\n",
    "        axs[i].imshow(img, cmap='gray')\n",
    "        axs[i].set_title(titles[i])\n",
    "        axs[i].axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    plt.savefig(os.path.join(save_dir, f\"epoch_{epoch}_viz.png\"))\n",
    "    plt.close()  # Close the plot to free memory\n",
    "\n",
    "def load_data(train_dir, val_dir, test_size, batch_size, num_workers_tr, num_workers_val, crop_size = None):\n",
    "    train_noisy = sorted([os.path.join(train_dir, \"RAW\", f) for f in os.listdir(os.path.join(train_dir, \"RAW\")) if f.endswith('.tif')])\n",
    "    train_gt = sorted([os.path.join(train_dir, \"GT\", f) for f in os.listdir(os.path.join(train_dir, \"GT\")) if f.endswith('.tif')])\n",
    "    val_noisy = sorted([os.path.join(val_dir, \"RAW\", f) for f in os.listdir(os.path.join(val_dir, \"RAW\")) if f.endswith('.tif')])\n",
    "    val_gt = sorted([os.path.join(val_dir, \"GT\", f) for f in os.listdir(os.path.join(val_dir, \"GT\")) if f.endswith('.tif')])\n",
    "\n",
    "    print(f\"{len(train_noisy)} training images and {len(val_noisy)} validation images after split.\")\n",
    "\n",
    "    train_ds = DenoisingDataset2D(train_noisy, train_gt, crop_size=crop_size, augment=True)\n",
    "    val_ds = DenoisingDataset2D(val_noisy, val_gt, crop_size=crop_size, augment=False)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers_tr, pin_memory=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=32, shuffle=False, num_workers=num_workers_val, pin_memory=True) # batch_size = 1\n",
    "\n",
    "    print(f\"\\nTraining DataLoader created with {len(train_loader)} batches.\")\n",
    "    print(f\"Validation DataLoader created with {len(val_loader)} batches.\")\n",
    "\n",
    "    return train_loader, val_loader\n",
    "\n",
    "# Define data paths (update these to your actual paths)\n",
    "train_dir = r\"D:\\Manuscipts_Coding\\Denoising_paper\\IgG-1D\\Exported_Data_TIFF\\train\"\n",
    "val_dir = r\"D:\\Manuscipts_Coding\\Denoising_paper\\IgG-1D\\Exported_Data_TIFF\\val\"\n",
    "pretrained_path = \"DATASET_7_NAFNet_GAN_best_model_LOSS_2_V2.pth\"  # If it does not exist, then it will not load the weights on train.\n",
    "checkpoint_path = \"DATASET_7_NAFNet_GAN_best_model_LOSS_2_V2.pth\"\n",
    "crop_size = 128\n",
    "num_workers_tr = 0 # Number of workers for training \n",
    "num_workers_val = 0 # Number of workers for validation \n",
    "\n",
    "# Step 1: Load data\n",
    "if os.path.exists(train_dir) == True and os.path.exists(val_dir) == True:\n",
    "    train_loader, val_loader = load_data(\n",
    "        train_dir=train_dir,\n",
    "        val_dir=val_dir,\n",
    "        test_size=0.04,\n",
    "        batch_size=32,\n",
    "        num_workers_tr=num_workers_tr,  \n",
    "        num_workers_val=num_workers_val,  \n",
    "        crop_size=crop_size\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03882ee-680c-465d-96af-c82c25ae2614",
   "metadata": {},
   "source": [
    "### Step 2: Visualize the dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca94cf1e-476a-4593-a278-08b8e03e02f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Visualize a sample from training and validation data\n",
    "def visualize_dataloader(loader, title=\"Sample\"):\n",
    "    start_time = time.perf_counter()\n",
    "    noisy_img_batch, gt_img_batch = next(iter(loader))\n",
    "    end_time = time.perf_counter()\n",
    "    print(f\"Time to load first batch: {end_time - start_time:.4f} seconds\")\n",
    "    print(f\"Noisy batch shape: {noisy_img_batch.shape}, GT batch shape: {gt_img_batch.shape}\")\n",
    "\n",
    "    noisy_sample = noisy_img_batch[0].squeeze().cpu().numpy()\n",
    "    gt_sample = gt_img_batch[0].squeeze().cpu().numpy()\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    im0 = axs[0].imshow(noisy_sample, cmap='gray')\n",
    "    axs[0].set_title(f\"Noisy Input {title}\")\n",
    "    axs[0].axis(\"off\")\n",
    "    fig.colorbar(im0, ax=axs[0], fraction=0.046, pad=0.04)\n",
    "    im1 = axs[1].imshow(gt_sample, cmap='gray')\n",
    "    axs[1].set_title(f\"Ground Truth {title}\")\n",
    "    axs[1].axis(\"off\")\n",
    "    fig.colorbar(im1, ax=axs[1], fraction=0.046, pad=0.04)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"Visualizing training data sample:\")\n",
    "visualize_dataloader(train_loader, title=\"Training Sample (Cropped)\")\n",
    "\n",
    "print(\"Visualizing validation data sample:\")\n",
    "visualize_dataloader(val_loader, title=\"Validation Sample (Full)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fa12f9-bb02-46b4-bec2-4a86fcb2fdeb",
   "metadata": {},
   "source": [
    "### Step 3: Train the model.\n",
    "\n",
    "Note that we have only trained it for 1 epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3273a64-e892-402a-bd61-36f15d0fab0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from losses import MasterLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f873946b-189e-48e4-a40c-7a9f21d94fb2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TORCH_COMPILE_DISABLE'] = '1'\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.cuda.amp import GradScaler \n",
    "from torch.amp import autocast \n",
    "import warnings\n",
    "import time\n",
    "\n",
    "# --- Setup & Helper Functions ---\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def plot_images(noisy, pred, target, epoch, save_dir=\"training_visuals\"):\n",
    "    \"\"\"Saves and displays the input, predicted, and ground truth images with colorbars.\"\"\"\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    titles = [\"Noisy Input\", \"Denoised Output\", \"Ground Truth\"]\n",
    "    \n",
    "    # Move to CPU and detach for plotting\n",
    "    images = [\n",
    "        noisy.squeeze().cpu().detach().numpy(),\n",
    "        pred.squeeze().cpu().detach().numpy(),\n",
    "        target.squeeze().cpu().detach().numpy()\n",
    "    ]\n",
    "    \n",
    "    for i, img in enumerate(images):\n",
    "        im = axs[i].imshow(img, cmap='gray', vmin=0, vmax=1)\n",
    "        axs[i].set_title(f\"{titles[i]} (Epoch {epoch})\")\n",
    "        axs[i].axis(\"off\")\n",
    "        fig.colorbar(im, ax=axs[i], fraction=0.046, pad=0.04)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Assuming NAFNet, MasterLoss, LossWeights, train_loader, val_loader are defined/imported\n",
    "# If running as standalone script, ensure imports for these are present.\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- Model Setup ---\n",
    "img_channel = 1  # Grayscale\n",
    "width = 16\n",
    "enc_blks = [2, 2, 4, 8]\n",
    "middle_blk_num = 12\n",
    "dec_blks = [2, 2, 2, 2]\n",
    "\n",
    "# Initialize Generator (NAFNet)\n",
    "generator = NAFNet(\n",
    "    img_channel=img_channel,\n",
    "    width=width,\n",
    "    middle_blk_num=middle_blk_num,\n",
    "    enc_blk_nums=enc_blks,\n",
    "    dec_blk_nums=dec_blks\n",
    ").to(device)\n",
    "\n",
    "# Initialize Discriminator\n",
    "discriminator = Discriminator(in_channels=1).to(device)\n",
    "\n",
    "# Paths\n",
    "checkpoint_path = 'NAFNet_GAN_LVUP_Dataset_7_Conf-het_Best_Loss_3.pth'\n",
    "os.makedirs(\"training_visuals\", exist_ok=True)\n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs = 30\n",
    "T_max = max(1, int(num_epochs / 5)) \n",
    "\n",
    "# Optimizers & Schedulers\n",
    "g_optimizer = optim.Adam(generator.parameters(), lr=1e-4, betas=(0.5, 0.999))\n",
    "d_optimizer = optim.Adam(discriminator.parameters(), lr=1e-4, betas=(0.5, 0.999))\n",
    "\n",
    "g_scheduler = CosineAnnealingLR(g_optimizer, T_max=T_max)\n",
    "d_scheduler = CosineAnnealingLR(d_optimizer, T_max=T_max)\n",
    "\n",
    "# --- Loss Configuration ---\n",
    "# IMPORTANT: Using 'enhanced_deblur' for GAN training\n",
    "LOSS_TYPE = 'enhanced_deblur' \n",
    "\n",
    "class LossWeights:\n",
    "    lambda_lpips = 3.00    \n",
    "    lambda_vgg = 0.00     \n",
    "    lambda_charb = 0.5    \n",
    "    lambda_ssim = 0.0     \n",
    "    lambda_lap = 2.00     \n",
    "    lambda_edge = 0.0     \n",
    "    lambda_fft_cc = 0.5 \n",
    "    lambda_fft = 0.0     \n",
    "    lambda_gan = 0.5     \n",
    "    r1_gamma = 0.0       \n",
    "\n",
    "criterion = MasterLoss(loss_type=LOSS_TYPE, weights=LossWeights(), device=device)\n",
    "\n",
    "# Scalers for Mixed Precision\n",
    "scaler_g = GradScaler()\n",
    "scaler_d = GradScaler()\n",
    "\n",
    "# Tracking\n",
    "best_val_loss = float('inf')\n",
    "train_g_losses = []\n",
    "train_d_losses = []\n",
    "val_g_losses = []\n",
    "\n",
    "# --- PRE-TRAINING SETUP: Fixed Validation Batch ---\n",
    "# Grab one batch to use for consistent visualization throughout training\n",
    "print(\"Creating fixed validation batch for consistency...\")\n",
    "try:\n",
    "    fixed_val_input, fixed_val_target = next(iter(val_loader))\n",
    "    fixed_val_input = fixed_val_input.to(device)\n",
    "    fixed_val_target = fixed_val_target.to(device)\n",
    "    print(\"Fixed batch created successfully.\")\n",
    "except StopIteration:\n",
    "    print(\"Error: Validation loader is empty!\")\n",
    "    exit()\n",
    "\n",
    "# --- TRAINING LOOP ---\n",
    "for epoch in range(num_epochs):\n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "    \n",
    "    current_g_loss = 0.0\n",
    "    current_d_loss = 0.0\n",
    "    valid_batches = 0\n",
    "\n",
    "    # Train Step\n",
    "    pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Train]')\n",
    "    for noisy_img, clean_img in pbar:\n",
    "        noisy_img, clean_img = noisy_img.to(device), clean_img.to(device)\n",
    "\n",
    "        # NaN Check\n",
    "        if torch.isnan(noisy_img).any() or torch.isinf(noisy_img).any():\n",
    "            continue\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "        d_optimizer.zero_grad()\n",
    "        with autocast(device_type=device.type):\n",
    "            fake_img = generator(noisy_img)\n",
    "            fake_img = torch.clamp(fake_img, 0, 1) # Enforce [0,1] range\n",
    "            \n",
    "            # Real vs Fake inputs\n",
    "            d_real = discriminator(clean_img)\n",
    "            d_fake = discriminator(fake_img.detach()) # Detach to stop gradient to Generator\n",
    "            \n",
    "            d_loss = criterion.forward_discriminator(d_real, d_fake)\n",
    "\n",
    "        if torch.isnan(d_loss) or torch.isinf(d_loss):\n",
    "            continue\n",
    "\n",
    "        scaler_d.scale(d_loss).backward()\n",
    "        torch.nn.utils.clip_grad_norm_(discriminator.parameters(), max_norm=1.0)\n",
    "        scaler_d.step(d_optimizer)\n",
    "        scaler_d.update()\n",
    "\n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "        g_optimizer.zero_grad()\n",
    "        with autocast(device_type=device.type):\n",
    "            # Re-compute D output for Generator update (gradients flow this time)\n",
    "            d_fake_for_g = discriminator(fake_img)\n",
    "            \n",
    "            # Dictionary inputs for MasterLoss\n",
    "            g_loss_inputs = {\n",
    "                'pred_img': fake_img,\n",
    "                'target_img': clean_img,\n",
    "                'd_fake_logits': d_fake_for_g\n",
    "            }\n",
    "            g_loss = criterion.forward_generator(g_loss_inputs)\n",
    "\n",
    "        if torch.isnan(g_loss) or torch.isinf(g_loss):\n",
    "            continue\n",
    "\n",
    "        scaler_g.scale(g_loss).backward()\n",
    "        torch.nn.utils.clip_grad_norm_(generator.parameters(), max_norm=1.0)\n",
    "        scaler_g.step(g_optimizer)\n",
    "        scaler_g.update()\n",
    "\n",
    "        current_g_loss += g_loss.item()\n",
    "        current_d_loss += d_loss.item()\n",
    "        valid_batches += 1\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({'G_Loss': g_loss.item(), 'D_Loss': d_loss.item()})\n",
    "\n",
    "    # Epoch Averages\n",
    "    if valid_batches > 0:\n",
    "        current_g_loss /= valid_batches\n",
    "        current_d_loss /= valid_batches\n",
    "    else:\n",
    "        current_g_loss = float('inf')\n",
    "        current_d_loss = float('inf')\n",
    "\n",
    "    # --- VALIDATION LOOP ---\n",
    "    generator.eval()\n",
    "    discriminator.eval()\n",
    "    current_val_g_loss = 0.0\n",
    "    valid_val_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        with autocast(device_type=device.type):\n",
    "            for val_input, val_target in tqdm(val_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Valid]'):\n",
    "                val_input = val_input.to(device)\n",
    "                val_target = val_target.to(device)\n",
    "                \n",
    "                val_output = generator(val_input)\n",
    "                val_output = torch.clamp(val_output, 0, 1)\n",
    "                \n",
    "                # We only care about G loss for validation metric\n",
    "                d_fake_val = discriminator(val_output)\n",
    "                \n",
    "                val_inputs = {\n",
    "                    'pred_img': val_output,\n",
    "                    'target_img': val_target,\n",
    "                    'd_fake_logits': d_fake_val\n",
    "                }\n",
    "                \n",
    "                val_loss = criterion.forward_generator(val_inputs)\n",
    "                \n",
    "                if not (torch.isnan(val_loss) or torch.isinf(val_loss)):\n",
    "                    current_val_g_loss += val_loss.item() * val_input.size(0)\n",
    "                    valid_val_batches += val_input.size(0)\n",
    "\n",
    "    if valid_val_batches > 0:\n",
    "        current_val_g_loss /= valid_val_batches\n",
    "    else:\n",
    "        current_val_g_loss = float('inf')\n",
    "\n",
    "    # --- VISUALIZATION (Using Fixed Batch) ---\n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        generator.eval()\n",
    "        with torch.no_grad():\n",
    "            # Use the SAME fixed batch we grabbed at start\n",
    "            fixed_pred = generator(fixed_val_input)\n",
    "            fixed_pred = torch.clamp(fixed_pred, 0, 1)\n",
    "            \n",
    "            # Plot the first image from the fixed batch\n",
    "            plot_images(fixed_val_input[0], fixed_pred[0], fixed_val_target[0], epoch+1)\n",
    "\n",
    "    # --- LOGGING & SAVING ---\n",
    "    train_g_losses.append(current_g_loss)\n",
    "    train_d_losses.append(current_d_loss)\n",
    "    val_g_losses.append(current_val_g_loss)\n",
    "\n",
    "    print(f'Epoch {epoch + 1:02d} | Train G: {current_g_loss:.4f} | Train D: {current_d_loss:.4f} | Val G: {current_val_g_loss:.4f}')\n",
    "\n",
    "    # Save Best Model\n",
    "    if current_val_g_loss < best_val_loss and valid_val_batches > 0:\n",
    "        best_val_loss = current_val_g_loss\n",
    "        print(f\"--> New Best Model! Loss: {best_val_loss:.4f}\")\n",
    "        torch.save({\n",
    "            'generator_state_dict': generator.state_dict(),\n",
    "            'discriminator_state_dict': discriminator.state_dict(),\n",
    "            'g_optimizer_state_dict': g_optimizer.state_dict(),\n",
    "            'd_optimizer_state_dict': d_optimizer.state_dict(),\n",
    "            'val_loss': best_val_loss,\n",
    "        }, checkpoint_path)\n",
    "\n",
    "    # Save Latest Checkpoint\n",
    "    torch.save({\n",
    "        'generator_state_dict': generator.state_dict(),\n",
    "        'discriminator_state_dict': discriminator.state_dict(),\n",
    "        'g_optimizer_state_dict': g_optimizer.state_dict(),\n",
    "        'd_optimizer_state_dict': d_optimizer.state_dict(),\n",
    "        'val_loss': current_val_g_loss,\n",
    "    }, 'NAFNet_GAN_LVUP_Dataset_7_Conf-het_Latest_Loss_3.pth')\n",
    "    \n",
    "    g_scheduler.step()\n",
    "    d_scheduler.step()\n",
    "\n",
    "# --- FINISH & SAVE CSV ---\n",
    "loss_df = pd.DataFrame({\n",
    "    'Epoch': range(1, num_epochs + 1),\n",
    "    'Train Generator Loss': train_g_losses,\n",
    "    'Train Discriminator Loss': train_d_losses,\n",
    "    'Val Generator Loss': val_g_losses\n",
    "})\n",
    "loss_df.to_csv('NAFNet_GAN_LVUP_Dataset_7_Conf-het_Loss_3.csv', index=False)\n",
    "print(\"Training Complete.\")\n",
    "# Final visualization\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "generator.load_state_dict(checkpoint['generator_state_dict'])\n",
    "generator.eval()\n",
    "\n",
    "val_input, val_target = next(iter(val_loader))\n",
    "val_input = val_input.to(device)\n",
    "val_target = val_target.to(device)\n",
    "with torch.no_grad():\n",
    "    val_output = generator(val_input)\n",
    "\n",
    "input_img = val_input[0].cpu()\n",
    "pred_img = val_output[0].cpu()\n",
    "target_img = val_target[0].cpu()\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title(\"Input Image (Noisy)\")\n",
    "plt.imshow(input_img.squeeze(), cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title(\"Predicted Image (Denoised)\")\n",
    "plt.imshow(pred_img.squeeze(), cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title(\"Ground Truth (Clean)\")\n",
    "plt.imshow(target_img.squeeze(), cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.savefig(os.path.join(\"training_visuals\", \"final_result.png\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dbb84a-b6de-4c1d-aff3-5fae79b8d32a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad06fd80-e9ba-4c9b-b98b-434e4d871146",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af532f5c-20e5-4139-a6ad-8ed87e1e61c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from skimage.io import imread\n",
    "from skimage.filters import gaussian\n",
    "from skimage.util import random_noise\n",
    "import random\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class DenoisingDataset2D(Dataset):\n",
    "    def __init__(self, gt_paths, crop_size=None, augment=True, mode=\"train\", multiple=16, blind_ratio=0.0):\n",
    "        self.gt_paths = gt_paths\n",
    "        self.crop_size = crop_size\n",
    "        self.augment = augment\n",
    "        self.mode = mode\n",
    "        self.multiple = multiple\n",
    "        self.blind_ratio = blind_ratio  # NEW: Percentage of pixels to blind (e.g., 0.01)\n",
    "        \n",
    "        # Fixed validation schedule\n",
    "        if self.mode == \"val\":\n",
    "            n = len(gt_paths)\n",
    "            n_blur = int(0.4 * n)\n",
    "            n_noise = int(0.4 * n)\n",
    "            n_clean = n - n_blur - n_noise\n",
    "            self.val_aug_types = ([\"blur\"] * n_blur + [\"noise\"] * n_noise + [\"none\"] * n_clean)\n",
    "            random.shuffle(self.val_aug_types)\n",
    "\n",
    "    def pad_to_multiple(self, img, multiple=16):\n",
    "        \"\"\"Mirrors image edges to reach the next multiple of 'multiple'.\"\"\"\n",
    "        h, w = img.shape\n",
    "        pad_h = (multiple - (h % multiple)) % multiple\n",
    "        pad_w = (multiple - (w % multiple)) % multiple\n",
    "        \n",
    "        if pad_h > 0 or pad_w > 0:\n",
    "            top = pad_h // 2\n",
    "            bottom = pad_h - top\n",
    "            left = pad_w // 2\n",
    "            right = pad_w - left\n",
    "            img = np.pad(img, ((top, bottom), (left, right)), mode='reflect')\n",
    "        return img\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 1. Load Clean Ground Truth (Which is actually Raw in Self-Supervised)\n",
    "        gt = imread(self.gt_paths[idx]).astype(np.float32)\n",
    "        \n",
    "        # Normalize\n",
    "        gt = (gt - gt.min()) / (gt.max() - gt.min() + 1e-8)\n",
    "        \n",
    "        # 2. Create Noisy Input\n",
    "        noisy = gt.copy()\n",
    "        \n",
    "        # Determine Augmentation\n",
    "        aug_type = \"none\"\n",
    "        if self.mode == \"train\":\n",
    "            aug_type = random.choice([\"blur\", \"noise\", \"none\"])\n",
    "        elif self.mode == \"val\":\n",
    "            aug_type = self.val_aug_types[idx]\n",
    "\n",
    "        # Apply Degradation (Noisier2Noise / Noise2Blur logic)\n",
    "        if aug_type == \"blur\":\n",
    "            sigma = random.uniform(2, 5) if self.mode == \"train\" else 3\n",
    "            noisy = gaussian(noisy, sigma=sigma)\n",
    "        elif aug_type == \"noise\":\n",
    "            var = random.uniform(0.01, 0.1) if self.mode == \"train\" else 0.05\n",
    "            noisy = random_noise(noisy, mode=\"gaussian\", var=var)\n",
    "\n",
    "        # 3. APPLY BLINDING (MASKING) - NEW STEP\n",
    "        # Mask: 1 = Blinded (Calculate Loss Here), 0 = Kept (Ignore Loss Here)\n",
    "        mask = np.zeros_like(noisy, dtype=np.float32)\n",
    "        \n",
    "        if self.blind_ratio > 0:\n",
    "            # Create random boolean mask\n",
    "            blind_mask = np.random.random(noisy.shape) < self.blind_ratio\n",
    "            \n",
    "            # Generate random noise to fill the blinded spots \n",
    "            # (Using image statistics so it's not obvious zeros)\n",
    "            noise_fill = np.random.normal(loc=noisy.mean(), scale=noisy.std(), size=noisy.shape)\n",
    "            \n",
    "            # Apply blinding to input\n",
    "            noisy[blind_mask] = noise_fill[blind_mask]\n",
    "            \n",
    "            # Set mask to 1 where we blinded\n",
    "            mask[blind_mask] = 1.0\n",
    "\n",
    "        # 4. Dimension Fixing (Padding/Cropping)\n",
    "        if self.mode == \"train\" and self.crop_size:\n",
    "            h, w = noisy.shape\n",
    "            # Pad if needed\n",
    "            if h < self.crop_size or w < self.crop_size:\n",
    "                pad_h = max(0, self.crop_size - h)\n",
    "                pad_w = max(0, self.crop_size - w)\n",
    "                if pad_h > 0 or pad_w > 0:\n",
    "                    noisy = np.pad(noisy, ((0, pad_h), (0, pad_w)), mode='reflect')\n",
    "                    gt = np.pad(gt, ((0, pad_h), (0, pad_w)), mode='reflect')\n",
    "                    mask = np.pad(mask, ((0, pad_h), (0, pad_w)), mode='constant', constant_values=0) # Pad mask with 0\n",
    "                h, w = noisy.shape\n",
    "\n",
    "            # Random Crop\n",
    "            x = random.randint(0, h - self.crop_size)\n",
    "            y = random.randint(0, w - self.crop_size)\n",
    "            \n",
    "            noisy = noisy[x:x+self.crop_size, y:y+self.crop_size]\n",
    "            gt = gt[x:x+self.crop_size, y:y+self.crop_size]\n",
    "            mask = mask[x:x+self.crop_size, y:y+self.crop_size]\n",
    "            \n",
    "            # Flips\n",
    "            if self.augment:\n",
    "                if random.random() < 0.5:\n",
    "                    noisy = np.fliplr(noisy); gt = np.fliplr(gt); mask = np.fliplr(mask)\n",
    "                if random.random() < 0.5:\n",
    "                    noisy = np.flipud(noisy); gt = np.flipud(gt); mask = np.flipud(mask)\n",
    "\n",
    "        else:\n",
    "            # Validation Padding\n",
    "            noisy = self.pad_to_multiple(noisy, self.multiple)\n",
    "            gt = self.pad_to_multiple(gt, self.multiple)\n",
    "            mask = self.pad_to_multiple(mask, self.multiple)\n",
    "\n",
    "        # 5. Convert to Tensor\n",
    "        # Copy ensures no negative stride issues from flipping\n",
    "        noisy = torch.from_numpy(noisy.copy()).unsqueeze(0).float()\n",
    "        gt = torch.from_numpy(gt.copy()).unsqueeze(0).float()\n",
    "        mask = torch.from_numpy(mask.copy()).unsqueeze(0).float()\n",
    "        \n",
    "        return noisy, gt, mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.gt_paths)\n",
    "\n",
    "# --- Loader Function Update ---\n",
    "def load_data(gt_dir, batch_size, num_workers_tr, num_workers_val, crop_size=None):\n",
    "    train_gt = sorted([os.path.join(gt_dir, \"Patch_Train\", f) \n",
    "                       for f in os.listdir(os.path.join(gt_dir, \"Patch_Train\")) if f.endswith('.tif')])\n",
    "    val_gt = sorted([os.path.join(gt_dir, \"Patch_Val\", f) \n",
    "                     for f in os.listdir(os.path.join(gt_dir, \"Patch_Val\")) if f.endswith('.tif')])\n",
    "\n",
    "    # Set blind_ratio here (e.g., 0.02 for 2% blinding, typical for N2V is 0.2% - 2%)\n",
    "    train_ds = DenoisingDataset2D(train_gt, crop_size=crop_size, augment=True, mode=\"train\", blind_ratio=0.05)\n",
    "    val_ds = DenoisingDataset2D(val_gt, crop_size=crop_size, augment=False, mode=\"val\", blind_ratio=0.0) # Usually don't blind validation\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,\n",
    "                              num_workers=num_workers_tr, pin_memory=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=1, shuffle=False,\n",
    "                            num_workers=num_workers_val, pin_memory=True)\n",
    "\n",
    "    print(f\"{len(train_gt)} training images and {len(val_gt)} validation images.\")\n",
    "    return train_loader, val_loader\n",
    "\n",
    "gt_dir = r\"C:\\Users\\Guill\\Downloads\\EMPIAR_10197\\EMPIAR_10197\\10197\\data\\leginondata\\rawdata\\GT\"\n",
    "pretrained_path = \"DATASET_7_NAFNet_GAN_best_model_LOSS_2_Blind.pth\"  # If it does not exist, then it will not load the weights on train.\n",
    "checkpoint_path = \"DATASET_7_NAFNet_GAN_best_model_LOSS_2_Blind.pth\"\n",
    "crop_size = int(512)\n",
    "num_workers_tr = 0 # Number of workers for training \n",
    "num_workers_val = 0 # Number of workers for validation \n",
    "\n",
    "# Step 1: Load data\n",
    "if os.path.exists(gt_dir) == True:\n",
    "    train_loader, val_loader = load_data(\n",
    "        gt_dir=gt_dir,\n",
    "        batch_size=2,\n",
    "        num_workers_tr=num_workers_tr,  \n",
    "        num_workers_val=num_workers_val,  \n",
    "        crop_size=crop_size\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54e52ed-cf82-4fb6-8b96-967184cb21c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Visualize a sample from training and validation data\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_dataloader(loader, title=\"Sample\"):\n",
    "    start_time = time.perf_counter()\n",
    "    # Unpack 3 values: noisy, gt, mask\n",
    "    noisy_img_batch, gt_img_batch, mask_batch = next(iter(loader))\n",
    "    end_time = time.perf_counter()\n",
    "    print(f\"Time to load first batch: {end_time - start_time:.4f} seconds\")\n",
    "    print(f\"Noisy batch shape: {noisy_img_batch.shape}, GT batch shape: {gt_img_batch.shape}\")\n",
    "\n",
    "    noisy_sample = noisy_img_batch[0].squeeze().cpu().numpy()\n",
    "    gt_sample = gt_img_batch[0].squeeze().cpu().numpy()\n",
    "    mask_sample = mask_batch[0].squeeze().cpu().numpy()\n",
    "\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(15, 5)) # Plot 3 things now\n",
    "    \n",
    "    im0 = axs[0].imshow(noisy_sample, cmap='gray')\n",
    "    axs[0].set_title(f\"Noisy Input (Blinded) {title}\")\n",
    "    axs[0].axis(\"off\")\n",
    "    fig.colorbar(im0, ax=axs[0], fraction=0.046, pad=0.04)\n",
    "    \n",
    "    im1 = axs[1].imshow(gt_sample, cmap='gray')\n",
    "    axs[1].set_title(f\"Ground Truth {title}\")\n",
    "    axs[1].axis(\"off\")\n",
    "    fig.colorbar(im1, ax=axs[1], fraction=0.046, pad=0.04)\n",
    "    \n",
    "    # Visualize the Mask too so you can see where the blind spots are\n",
    "    im2 = axs[2].imshow(mask_sample, cmap='gray')\n",
    "    axs[2].set_title(f\"Blind Mask {title}\")\n",
    "    axs[2].axis(\"off\")\n",
    "    fig.colorbar(im2, ax=axs[2], fraction=0.046, pad=0.04)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"Visualizing training data sample:\")\n",
    "visualize_dataloader(train_loader, title=\"Training Sample (Cropped)\")\n",
    "\n",
    "print(\"Visualizing validation data sample:\")\n",
    "visualize_dataloader(val_loader, title=\"Validation Sample (Full)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c71016-c9e5-4fa9-9b14-234b3ec035be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TORCH_COMPILE_DISABLE'] = '1'\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.cuda.amp import GradScaler \n",
    "from torch.amp import autocast \n",
    "import warnings\n",
    "import time\n",
    "\n",
    "# --- Setup & Helper Functions ---\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def plot_images(noisy, pred, target, epoch, save_dir=\"training_visuals\"):\n",
    "    \"\"\"Saves and displays the input, predicted, and ground truth images with colorbars.\"\"\"\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    titles = [\"Noisy Input\", \"Denoised Output\", \"Ground Truth\"]\n",
    "    \n",
    "    # Move to CPU and detach for plotting\n",
    "    images = [\n",
    "        noisy.squeeze().cpu().detach().numpy(),\n",
    "        pred.squeeze().cpu().detach().numpy(),\n",
    "        target.squeeze().cpu().detach().numpy()\n",
    "    ]\n",
    "    \n",
    "    for i, img in enumerate(images):\n",
    "        im = axs[i].imshow(img, cmap='gray', vmin=0, vmax=1)\n",
    "        axs[i].set_title(f\"{titles[i]} (Epoch {epoch})\")\n",
    "        axs[i].axis(\"off\")\n",
    "        fig.colorbar(im, ax=axs[i], fraction=0.046, pad=0.04)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Assuming NAFNet, MasterLoss, LossWeights, train_loader, val_loader are defined/imported\n",
    "# If running as standalone script, ensure imports for these are present.\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- Model Setup ---\n",
    "img_channel = 1  # Grayscale\n",
    "width = 16\n",
    "enc_blks = [2, 2, 4, 8]\n",
    "middle_blk_num = 12\n",
    "dec_blks = [2, 2, 2, 2]\n",
    "\n",
    "# Initialize Generator (NAFNet)\n",
    "generator = NAFNet(\n",
    "    img_channel=img_channel,\n",
    "    width=width,\n",
    "    middle_blk_num=middle_blk_num,\n",
    "    enc_blk_nums=enc_blks,\n",
    "    dec_blk_nums=dec_blks\n",
    ").to(device)\n",
    "\n",
    "# Initialize Discriminator\n",
    "discriminator = Discriminator(in_channels=1).to(device)\n",
    "\n",
    "# Paths\n",
    "checkpoint_path = 'NAFNet_GAN_LVUP_Dataset_7_Best_Loss_3_Blind.pth'\n",
    "os.makedirs(\"training_visuals\", exist_ok=True)\n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs = 100\n",
    "T_max = max(1, int(num_epochs / 5)) \n",
    "\n",
    "# Optimizers & Schedulers\n",
    "g_optimizer = optim.Adam(generator.parameters(), lr=1e-3, betas=(0.5, 0.999))\n",
    "d_optimizer = optim.Adam(discriminator.parameters(), lr=1e-3, betas=(0.5, 0.999))\n",
    "\n",
    "g_scheduler = CosineAnnealingLR(g_optimizer, T_max=T_max)\n",
    "d_scheduler = CosineAnnealingLR(d_optimizer, T_max=T_max)\n",
    "\n",
    "# --- Loss Configuration ---\n",
    "# IMPORTANT: Using 'enhanced_deblur' for GAN training\n",
    "LOSS_TYPE = 'enhanced_deblur' \n",
    "\n",
    "class LossWeights:\n",
    "    lambda_lpips = 3.00     \n",
    "    lambda_vgg = 0.00      \n",
    "    lambda_charb = 0.5     \n",
    "    lambda_ssim = 0.0      \n",
    "    lambda_lap = 2.00      \n",
    "    lambda_edge = 0.0      \n",
    "    lambda_fft_cc = 0.5 \n",
    "    lambda_fft = 0.0      \n",
    "    lambda_gan = 0.5      \n",
    "    r1_gamma = 0.0        \n",
    "\n",
    "criterion = MasterLoss(loss_type=LOSS_TYPE, weights=LossWeights(), device=device)\n",
    "\n",
    "# Scalers for Mixed Precision\n",
    "scaler_g = GradScaler()\n",
    "scaler_d = GradScaler()\n",
    "\n",
    "# Tracking\n",
    "best_val_loss = float('inf')\n",
    "train_g_losses = []\n",
    "train_d_losses = []\n",
    "val_g_losses = []\n",
    "\n",
    "# --- PRE-TRAINING SETUP: Fixed Validation Batch ---\n",
    "# Grab one batch to use for consistent visualization throughout training\n",
    "print(\"Creating fixed validation batch for consistency...\")\n",
    "try:\n",
    "    # UPDATED: Expect 3 values\n",
    "    fixed_val_input, fixed_val_target, fixed_val_mask = next(iter(val_loader))\n",
    "    fixed_val_input = fixed_val_input.to(device)\n",
    "    fixed_val_target = fixed_val_target.to(device)\n",
    "    # fixed_val_mask is likely mostly 0s for validation if blind_ratio=0, but kept for compatibility\n",
    "    print(\"Fixed batch created successfully.\")\n",
    "except StopIteration:\n",
    "    print(\"Error: Validation loader is empty!\")\n",
    "    exit()\n",
    "\n",
    "# --- TRAINING LOOP ---\n",
    "for epoch in range(num_epochs):\n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "    \n",
    "    current_g_loss = 0.0\n",
    "    current_d_loss = 0.0\n",
    "    valid_batches = 0\n",
    "\n",
    "    # Train Step\n",
    "    pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Train]')\n",
    "    \n",
    "    # UPDATED LOOP: Unpack 3 items\n",
    "    for noisy_img, clean_img, mask in pbar:\n",
    "        noisy_img = noisy_img.to(device)\n",
    "        clean_img = clean_img.to(device)\n",
    "        mask = mask.to(device)\n",
    "\n",
    "        # NaN Check\n",
    "        if torch.isnan(noisy_img).any() or torch.isinf(noisy_img).any():\n",
    "            continue\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "        d_optimizer.zero_grad()\n",
    "        with autocast(device_type=device.type):\n",
    "            fake_img = generator(noisy_img)\n",
    "            fake_img = torch.clamp(fake_img, 0, 1) # Enforce [0,1] range\n",
    "            \n",
    "            # Real vs Fake inputs\n",
    "            d_real = discriminator(clean_img)\n",
    "            d_fake = discriminator(fake_img.detach()) # Detach to stop gradient to Generator\n",
    "            \n",
    "            d_loss = criterion.forward_discriminator(d_real, d_fake)\n",
    "\n",
    "        if torch.isnan(d_loss) or torch.isinf(d_loss):\n",
    "            continue\n",
    "\n",
    "        scaler_d.scale(d_loss).backward()\n",
    "        torch.nn.utils.clip_grad_norm_(discriminator.parameters(), max_norm=1.0)\n",
    "        scaler_d.step(d_optimizer)\n",
    "        scaler_d.update()\n",
    "\n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "        g_optimizer.zero_grad()\n",
    "        with autocast(device_type=device.type):\n",
    "            # Re-compute D output for Generator update (gradients flow this time)\n",
    "            d_fake_for_g = discriminator(fake_img)\n",
    "            \n",
    "            # --- CRITICAL CHANGE: MASKING THE LOSS ---\n",
    "            # Only calculate loss where mask == 1 (the blinded pixels)\n",
    "            # Note: GAN loss usually applies to the whole image structure, \n",
    "            # but reconstruction loss (L1/Charbonnier) strictly needs the mask.\n",
    "            # MasterLoss might need adjustment if it calculates L1 internally.\n",
    "            # Ideally, pass masked images to MasterLoss if supported, or apply mask here.\n",
    "            \n",
    "            # For simplicity in this script, we assume MasterLoss handles the weighted sum.\n",
    "            # We pass masked inputs where strictly pixel-wise comparison is needed.\n",
    "            # However, since MasterLoss is a black box here, passing the full images is standard\n",
    "            # UNLESS you modify MasterLoss. \n",
    "            \n",
    "            # If MasterLoss is NOT modified to handle masks, simply passing full images \n",
    "            # trains strictly on the \"Noisier2Noise\" task (input has extra noise, target is original noisy).\n",
    "            # The \"Blind Spot\" constraint is implicitly handled because the input pixels are corrupted.\n",
    "            \n",
    "            g_loss_inputs = {\n",
    "                'pred_img': fake_img,\n",
    "                'target_img': clean_img, \n",
    "                'd_fake_logits': d_fake_for_g,\n",
    "                'mask': mask # Passing mask in case MasterLoss uses it (Optional if MasterLoss ignores it)\n",
    "            }\n",
    "            \n",
    "            # If MasterLoss does NOT support 'mask', the training still works as a \n",
    "            # Denoising Autoencoder (DAE) because the input is corrupted (blinded) \n",
    "            # and the target is the original.\n",
    "            \n",
    "            g_loss = criterion.forward_generator(g_loss_inputs)\n",
    "\n",
    "        if torch.isnan(g_loss) or torch.isinf(g_loss):\n",
    "            continue\n",
    "\n",
    "        scaler_g.scale(g_loss).backward()\n",
    "        torch.nn.utils.clip_grad_norm_(generator.parameters(), max_norm=1.0)\n",
    "        scaler_g.step(g_optimizer)\n",
    "        scaler_g.update()\n",
    "\n",
    "        current_g_loss += g_loss.item()\n",
    "        current_d_loss += d_loss.item()\n",
    "        valid_batches += 1\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({'G_Loss': g_loss.item(), 'D_Loss': d_loss.item()})\n",
    "\n",
    "    # Epoch Averages\n",
    "    if valid_batches > 0:\n",
    "        current_g_loss /= valid_batches\n",
    "        current_d_loss /= valid_batches\n",
    "    else:\n",
    "        current_g_loss = float('inf')\n",
    "        current_d_loss = float('inf')\n",
    "\n",
    "    # --- VALIDATION LOOP ---\n",
    "    generator.eval()\n",
    "    discriminator.eval()\n",
    "    current_val_g_loss = 0.0\n",
    "    valid_val_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        with autocast(device_type=device.type):\n",
    "            # UPDATED LOOP: Unpack 3 items\n",
    "            for val_input, val_target, val_mask in tqdm(val_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Valid]'):\n",
    "                val_input = val_input.to(device)\n",
    "                val_target = val_target.to(device)\n",
    "                \n",
    "                val_output = generator(val_input)\n",
    "                val_output = torch.clamp(val_output, 0, 1)\n",
    "                \n",
    "                d_fake_val = discriminator(val_output)\n",
    "                \n",
    "                val_inputs = {\n",
    "                    'pred_img': val_output,\n",
    "                    'target_img': val_target,\n",
    "                    'd_fake_logits': d_fake_val,\n",
    "                    'mask': val_mask # Pass mask just in case\n",
    "                }\n",
    "                \n",
    "                val_loss = criterion.forward_generator(val_inputs)\n",
    "                \n",
    "                if not (torch.isnan(val_loss) or torch.isinf(val_loss)):\n",
    "                    current_val_g_loss += val_loss.item() * val_input.size(0)\n",
    "                    valid_val_batches += val_input.size(0)\n",
    "\n",
    "    if valid_val_batches > 0:\n",
    "        current_val_g_loss /= valid_val_batches\n",
    "    else:\n",
    "        current_val_g_loss = float('inf')\n",
    "\n",
    "    # --- VISUALIZATION (Using Fixed Batch) ---\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        generator.eval()\n",
    "        with torch.no_grad():\n",
    "            # Use the SAME fixed batch we grabbed at start\n",
    "            fixed_pred = generator(fixed_val_input)\n",
    "            fixed_pred = torch.clamp(fixed_pred, 0, 1)\n",
    "            \n",
    "            # Plot the first image from the fixed batch\n",
    "            plot_images(fixed_val_input[0], fixed_pred[0], fixed_val_target[0], epoch+1)\n",
    "\n",
    "    # --- LOGGING & SAVING ---\n",
    "    train_g_losses.append(current_g_loss)\n",
    "    train_d_losses.append(current_d_loss)\n",
    "    val_g_losses.append(current_val_g_loss)\n",
    "\n",
    "    print(f'Epoch {epoch + 1:02d} | Train G: {current_g_loss:.4f} | Train D: {current_d_loss:.4f} | Val G: {current_val_g_loss:.4f}')\n",
    "\n",
    "    # Save Best Model\n",
    "    if current_val_g_loss < best_val_loss and valid_val_batches > 0:\n",
    "        best_val_loss = current_val_g_loss\n",
    "        print(f\"--> New Best Model! Loss: {best_val_loss:.4f}\")\n",
    "        torch.save({\n",
    "            'generator_state_dict': generator.state_dict(),\n",
    "            'discriminator_state_dict': discriminator.state_dict(),\n",
    "            'g_optimizer_state_dict': g_optimizer.state_dict(),\n",
    "            'd_optimizer_state_dict': d_optimizer.state_dict(),\n",
    "            'val_loss': best_val_loss,\n",
    "        }, checkpoint_path)\n",
    "\n",
    "    # Save Latest Checkpoint\n",
    "    torch.save({\n",
    "        'generator_state_dict': generator.state_dict(),\n",
    "        'discriminator_state_dict': discriminator.state_dict(),\n",
    "        'g_optimizer_state_dict': g_optimizer.state_dict(),\n",
    "        'd_optimizer_state_dict': d_optimizer.state_dict(),\n",
    "        'val_loss': current_val_g_loss,\n",
    "    }, 'NAFNet_GAN_LVUP_Dataset_7_Latest_Loss_3_Blind.pth')\n",
    "    \n",
    "    g_scheduler.step()\n",
    "    d_scheduler.step()\n",
    "\n",
    "# --- FINISH & SAVE CSV ---\n",
    "loss_df = pd.DataFrame({\n",
    "    'Epoch': range(1, num_epochs + 1),\n",
    "    'Train Generator Loss': train_g_losses,\n",
    "    'Train Discriminator Loss': train_d_losses,\n",
    "    'Val Generator Loss': val_g_losses\n",
    "})\n",
    "loss_df.to_csv('NAFNet_GAN_LVUP_Dataset_7_Loss_3_Blind.csv', index=False)\n",
    "print(\"Training Complete.\")\n",
    "\n",
    "# Final visualization\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "generator.load_state_dict(checkpoint['generator_state_dict'])\n",
    "generator.eval()\n",
    "\n",
    "# UPDATED: Unpack 3 items\n",
    "val_input, val_target, val_mask = next(iter(val_loader))\n",
    "val_input = val_input.to(device)\n",
    "val_target = val_target.to(device)\n",
    "with torch.no_grad():\n",
    "    val_output = generator(val_input)\n",
    "\n",
    "input_img = val_input[0].cpu()\n",
    "pred_img = val_output[0].cpu()\n",
    "target_img = val_target[0].cpu()\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title(\"Input Image (Noisy/Blinded)\")\n",
    "plt.imshow(input_img.squeeze(), cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title(\"Predicted Image (Denoised)\")\n",
    "plt.imshow(pred_img.squeeze(), cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title(\"Ground Truth (Target)\")\n",
    "plt.imshow(target_img.squeeze(), cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.savefig(os.path.join(\"training_visuals\", \"final_result.png\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311a98a4-812a-43c1-9a30-953fb51fa939",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f45f4e1-502a-4e91-868b-a640dde6f5d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef1484b-d89d-43d7-aa33-b1e74a4e8b59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5716e854-5fac-4e5a-96f6-7e6868880603",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a06f26f-52e0-42a5-9991-710ea11c485c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f591961-a933-48f1-b421-7d5a474c3236",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e8e10a-1ad0-4185-940c-d6fb91aaf4ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

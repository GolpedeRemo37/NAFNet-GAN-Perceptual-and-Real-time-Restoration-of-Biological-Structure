{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b93fb2-3ca4-45b9-bcee-21800c89b484",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage import io\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "# --- IMPORTS FROM YOUR FILES ---\n",
    "# Ensure models.py is accessible\n",
    "from models import NAFNet\n",
    "\n",
    "# --- YOUR DATASET CLASS (Copied here for standalone functionality) ---\n",
    "class DenoisingDataset2D(Dataset):\n",
    "    def __init__(self, noisy_paths, gt_paths, crop_size=None, augment=True, p=0.5):\n",
    "        assert len(noisy_paths) == len(gt_paths), \"Noisy and GT paths must have the same length\"\n",
    "        self.noisy_paths = noisy_paths\n",
    "        self.gt_paths = gt_paths\n",
    "        self.crop_size = crop_size\n",
    "        self.augment = augment\n",
    "        self.p = p\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        noisy = io.imread(self.noisy_paths[idx]).astype(np.float32)\n",
    "        gt = io.imread(self.gt_paths[idx]).astype(np.float32)\n",
    "        \n",
    "        # Normalize\n",
    "        noisy = (noisy - noisy.min()) / (noisy.max() - noisy.min() + 1e-8)\n",
    "        gt = (gt - gt.min()) / (gt.max() - gt.min() + 1e-8)\n",
    "        \n",
    "        # Augment (Skipped if augment=False)\n",
    "        if self.augment:\n",
    "            h, w = noisy.shape\n",
    "            crop_h = crop_w = self.crop_size \n",
    "            if h < crop_h or w < crop_w:\n",
    "                raise ValueError(f\"Image too small for augmentation crop: ({h}, {w}) at index {idx}\")\n",
    "            \n",
    "            max_x = h - crop_h\n",
    "            max_y = w - crop_w\n",
    "            x = random.randint(0, max_x) if max_x > 0 else 0\n",
    "            y = random.randint(0, max_y) if max_y > 0 else 0\n",
    "            noisy = noisy[x:x+crop_h, y:y+crop_w]\n",
    "            gt = gt[x:x+crop_h, y:y+crop_w]\n",
    "            \n",
    "            if random.random() < self.p:\n",
    "                noisy = np.fliplr(noisy).copy()\n",
    "                gt = np.fliplr(gt).copy()\n",
    "            if random.random() < self.p:\n",
    "                noisy = np.flipud(noisy).copy()\n",
    "                gt = np.flipud(gt).copy()\n",
    "        \n",
    "        # Convert to tensors\n",
    "        noisy = torch.from_numpy(noisy.copy()).unsqueeze(0)\n",
    "        gt = torch.from_numpy(gt.copy()).unsqueeze(0)\n",
    "        return noisy, gt\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.noisy_paths)\n",
    "\n",
    "# --- HELPER: PADDING FOR NAFNET ---\n",
    "def pad_to_multiple(x, multiple=32):\n",
    "    \"\"\"Pads image tensor to be divisible by 'multiple' (required for NAFNet).\"\"\"\n",
    "    h, w = x.shape[2], x.shape[3]\n",
    "    H = ((h + multiple - 1) // multiple) * multiple\n",
    "    W = ((w + multiple - 1) // multiple) * multiple\n",
    "    pad_h = H - h\n",
    "    pad_w = W - w\n",
    "    # Pad using reflection to minimize edge artifacts\n",
    "    x_padded = torch.nn.functional.pad(x, (0, pad_w, 0, pad_h), mode='reflect')\n",
    "    return x_padded, pad_h, pad_w\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "INPUT_DIR = r\"D:\\Manuscipts_Coding\\Denoising_paper\\IgG-1D\\Exported_Data_TIFF\\test\\RAW\"\n",
    "OUTPUT_DIR = r\"D:\\Manuscipts_Coding\\Denoising_paper\\Models\\Evaluation\\Preds_NafNet_GAN_LV3\\Dataset_7\"\n",
    "MODEL_PATH = \"NAFNet_GAN_LVUP_Dataset_7_Conf-het_Best_Loss_3.pth\" \n",
    "\n",
    "# Model Params (Must match training)\n",
    "IMG_CHANNEL = 1\n",
    "WIDTH = 16\n",
    "ENC_BLKS = [2, 2, 4, 8]\n",
    "MIDDLE_BLK_NUM = 12\n",
    "DEC_BLKS = [2, 2, 2, 2]\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def run_inference():\n",
    "    if not os.path.exists(OUTPUT_DIR):\n",
    "        os.makedirs(OUTPUT_DIR)\n",
    "    \n",
    "    # 1. Prepare Data Lists\n",
    "    # Get all .tif files\n",
    "    file_names = sorted([f for f in os.listdir(INPUT_DIR) if f.endswith('.tif')])\n",
    "    noisy_paths = [os.path.join(INPUT_DIR, f) for f in file_names]\n",
    "    \n",
    "    # TRICK: Use noisy paths as GT paths so the Dataset doesn't crash.\n",
    "    # We won't use the loaded GT, but the Dataset requires the list to exist.\n",
    "    gt_paths = noisy_paths \n",
    "\n",
    "    print(f\"Found {len(noisy_paths)} images.\")\n",
    "\n",
    "    # 2. Initialize Dataset & DataLoader\n",
    "    # CRITICAL: augment=False so it doesn't crop!\n",
    "    test_ds = DenoisingDataset2D(noisy_paths, gt_paths, crop_size=None, augment=False)\n",
    "    \n",
    "    # Batch size 1 is safest for inference on full-size images\n",
    "    test_loader = DataLoader(test_ds, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "    # 3. Load Model\n",
    "    print(\"Loading Model...\")\n",
    "    model = NAFNet(\n",
    "        img_channel=IMG_CHANNEL,\n",
    "        width=WIDTH,\n",
    "        middle_blk_num=MIDDLE_BLK_NUM,\n",
    "        enc_blk_nums=ENC_BLKS,\n",
    "        dec_blk_nums=DEC_BLKS\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    checkpoint = torch.load(MODEL_PATH, map_location=DEVICE)\n",
    "    if 'generator_state_dict' in checkpoint:\n",
    "        model.load_state_dict(checkpoint['generator_state_dict'])\n",
    "    else:\n",
    "        model.load_state_dict(checkpoint)\n",
    "    model.eval()\n",
    "\n",
    "    # 4. Inference Loop\n",
    "    print(\"Starting Inference...\")\n",
    "    \n",
    "    # zip(test_loader, file_names) lets us access the data AND the original filename\n",
    "    with torch.no_grad():\n",
    "        for (noisy, _), filename in tqdm(zip(test_loader, file_names), total=len(file_names)):\n",
    "            \n",
    "            noisy = noisy.to(DEVICE) # Shape: (1, 1, H, W)\n",
    "\n",
    "            # --- Handle Dimensions (Padding) ---\n",
    "            # NAFNet crashes if H, W aren't multiples of 32\n",
    "            noisy_padded, ph, pw = pad_to_multiple(noisy, multiple=32)\n",
    "\n",
    "            # --- Forward Pass ---\n",
    "            pred = model(noisy_padded)\n",
    "\n",
    "            # --- Un-Pad (Crop back to original) ---\n",
    "            if ph > 0 or pw > 0:\n",
    "                pred = pred[:, :, :pred.shape[2]-ph, :pred.shape[3]-pw]\n",
    "\n",
    "            # --- Post-Processing ---\n",
    "            pred = torch.clamp(pred, 0, 1)\n",
    "            pred_np = pred.squeeze().cpu().numpy() # Shape: (H, W)\n",
    "\n",
    "            # --- Save ---\n",
    "            save_path = os.path.join(OUTPUT_DIR, filename)\n",
    "            io.imsave(save_path, pred_np, check_contrast=False)\n",
    "\n",
    "    print(f\"\\nProcessing Complete. Saved to: {OUTPUT_DIR}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_inference()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
